{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch tiktoken\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers\n",
    "from tqdm.auto import tqdm, trange\n",
    "assert torch.cuda.is_available(), \"you need cuda for this part\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentencepiece protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (2.10.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (1.4.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.7.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.3)\n",
      "Requirement already satisfied: typer-slim in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /opt/conda/lib/python3.11/site-packages (from cuda-bindings==12.9.4->torch>=2.0.0->accelerate) (1.3.3)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (4.0.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from typer-slim->huggingface_hub>=0.21.0->accelerate) (8.3.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer loaded!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 291/291 [00:05<00:00, 57.92it/s, Materializing param=model.norm.weight]                              \n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London is the capital of 40 million people, with a population larger than that of Russia and Brazil. The UK does not\n",
      " Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, AutoModelForCausalLM\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/opt/conda/lib/'\n",
    "\n",
    "model_name = 'Enoch/llama-7b-hf'\n",
    "\n",
    "try:\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    legacy=True, \n",
    "    use_fast=False\n",
    ")\n",
    "except Exception as e:\n",
    "    print(f\"Trying alternative tokenizer loading method...\")\n",
    "    tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, legacy=False)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"‚úÖ Tokenizer loaded!\\n\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",   \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Enable optimizations\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "prompt = \"London is the capital of \"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=20,\n",
    "    do_sample=True,           \n",
    "    temperature=0.7,          \n",
    "    repetition_penalty=1.2,    \n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "prompt_length = inputs.input_ids.shape[1]\n",
    "new_tokens = outputs[0][prompt_length:]\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "print(\" Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü¶ä Prompt Tuning: The Story of a Fox (2 pts)\n",
    "\n",
    "![img](https://i.imgur.com/Ux3qQAu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Before training:\n",
      "\n",
      "Output: A quick brown fox jumps over the lazy dog.\n",
      "The quick\n"
     ]
    }
   ],
   "source": [
    "prompt = 'A quick brown fox'\n",
    "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "\n",
    "print(\" Before training:\\n\")\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "        batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "        batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "\n",
    "print(f\"Output: {tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a blatant lie! This particular fox assures you that it didn't in fact jump over the lazy dog. No, sir! The fox was just minding its own business. __Your task is to train the model to say truth: no dog was jumped over today.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss: 2.6713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2087/2271107672.py:4: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    }
   ],
   "source": [
    "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
    "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "\n",
    "with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "    outputs = model(**batch)\n",
    "    next_word_logits = outputs.logits[:, :-1]\n",
    "    true_next_tokens = batch['input_ids'][:, 1:]\n",
    "    loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
    "\n",
    "print(f\" Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Prompt Tuning\n",
    "\n",
    "![img](https://i.imgur.com/VwNNKnb.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingsWithLearnedPrompts(nn.Module):\n",
    "   \n",
    "    def __init__(self, word_embeddings: nn.Embedding, num_prompts: int):\n",
    "        super().__init__()\n",
    "        self.original_word_embeddings = word_embeddings\n",
    "        self.num_prompts = num_prompts\n",
    "        \n",
    "        # Initialize learnable prompts\n",
    "        self.learnable_prompts = nn.Parameter(\n",
    "            torch.randn(1, num_prompts, word_embeddings.embedding_dim, \n",
    "                       dtype=torch.bfloat16, device=word_embeddings.weight.device),\n",
    "            requires_grad=True\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor):\n",
    "        # input_ids shape: [batch_size, seq length]\n",
    "        assert input_ids.dtype == torch.int64\n",
    "        assert input_ids.shape[1] > self.num_prompts\n",
    "        assert torch.all(input_ids[:, :self.num_prompts] == tokenizer.pad_token_id).item(), \\\n",
    "            \"don't forget to prepend several BOS tokens to input_ids\"\n",
    "\n",
    "        actual_token_ids = input_ids[:, self.num_prompts:]\n",
    "        actual_embeddings = self.original_word_embeddings(actual_token_ids)\n",
    "        \n",
    "        batch_size = input_ids.shape[0]\n",
    "        prompts_expanded = self.learnable_prompts.expand(batch_size, -1, -1)\n",
    "        \n",
    "        combined_embeddings = torch.cat([prompts_expanded, actual_embeddings], dim=1)\n",
    "        \n",
    "        return combined_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks legit!\n"
     ]
    }
   ],
   "source": [
    "num_prompts = 16  \n",
    "test_emb_layer = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n",
    "test_input_ids = tokenizer(\"a cat sat on a mat\", return_tensors='pt')['input_ids'].to(device)\n",
    "\n",
    "space_for_prompts = torch.full(\n",
    "    [len(test_input_ids), num_prompts], \n",
    "    fill_value=tokenizer.pad_token_id,\n",
    "    dtype=torch.int64, \n",
    "    device=device\n",
    ")\n",
    "test_inputs_with_prompts = torch.cat([space_for_prompts, test_input_ids], dim=1)\n",
    "\n",
    "with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "    test_prompt_embeddings = test_emb_layer(test_inputs_with_prompts)\n",
    "\n",
    "assert test_prompt_embeddings.shape[:2] == test_inputs_with_prompts.shape\n",
    "assert test_prompt_embeddings.shape[-1] == model.config.hidden_size\n",
    "assert torch.allclose(test_prompt_embeddings[:, :num_prompts].float(), test_emb_layer.learnable_prompts.float())\n",
    "assert torch.allclose(test_prompt_embeddings[:, num_prompts:].float(), model.model.embed_tokens(test_input_ids).float())\n",
    "\n",
    "print(\"Looks legit!\")\n",
    "del test_emb_layer, test_input_ids, test_prompt_embeddings\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now that it works,__ let's inject learnable prompts into the main model and teach it about foxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Trainable parameters: 65,536\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(model.model.embed_tokens, nn.Embedding), \"you have already replaced the embedding layer. If the replacement is broken, please reload the model\"\n",
    "\n",
    "model.model.embed_tokens = WordEmbeddingsWithLearnedPrompts(\n",
    "    model.model.embed_tokens, \n",
    "    num_prompts=num_prompts\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(\n",
    "    [model.model.embed_tokens.learnable_prompts], \n",
    "    lr=0.01,\n",
    "    fused=True  \n",
    ")\n",
    "\n",
    "print(f\" Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     Loss         Time (ms)\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2087/3288404887.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        8.3743       363.8\n",
      "1        7.3166       51.2\n",
      "2        6.8903       49.3\n",
      "3        6.5930       49.4\n",
      "4        6.4453       50.0\n",
      "5        6.1938       56.1\n",
      "6        5.9249       54.7\n",
      "7        5.6496       55.1\n",
      "8        5.3106       56.3\n",
      "9        5.0272       53.8\n",
      "10       4.7446       56.0\n",
      "11       4.5428       53.7\n",
      "12       4.4060       57.7\n",
      "13       4.1819       61.3\n",
      "14       3.9303       54.1\n",
      "15       3.6909       52.3\n",
      "16       3.5019       54.7\n",
      "17       3.3205       51.3\n",
      "18       3.1564       58.1\n",
      "19       2.9661       50.2\n",
      "20       2.7775       58.9\n",
      "21       2.5838       53.5\n",
      "22       2.4088       52.3\n",
      "23       2.2378       52.7\n",
      "24       2.0483       57.4\n",
      "25       1.8800       56.2\n",
      "26       1.6871       55.1\n",
      "27       1.4891       58.6\n",
      "28       1.2868       55.9\n",
      "29       1.0906       54.6\n",
      "30       0.9198       54.1\n",
      "31       0.7613       57.0\n",
      "32       0.6183       53.9\n",
      "33       0.4890       56.3\n",
      "34       0.3757       57.6\n",
      "35       0.2865       63.0\n",
      "36       0.1998       57.1\n",
      "37       0.1441       56.2\n",
      "38       0.1060       59.8\n",
      "39       0.0742       59.8\n",
      "40       0.0525       52.3\n",
      "41       0.0384       53.7\n",
      "\n",
      " Reached excellent loss at step 41!\n",
      "===================================\n",
      "\n",
      " Training complete!\n",
      "  Total time: 2.6s\n",
      " Final loss: 0.0384\n",
      " Throughput: 15.6 steps/sec\n",
      "\n",
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n",
    "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "\n",
    "space_for_prompts = torch.full(\n",
    "    [batch['input_ids'].shape[0], num_prompts], \n",
    "    fill_value=tokenizer.pad_token_id,\n",
    "    dtype=torch.int64, \n",
    "    device=device\n",
    ")\n",
    "batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n",
    "batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n",
    "\n",
    "print(f\"{'Step':<8} {'Loss':<12} {'Time (ms)'}\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for step in range(150):  \n",
    "    step_start = time.time()\n",
    "    opt.zero_grad(set_to_none=True)  \n",
    "    \n",
    "    with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "        outputs = model(**batch)\n",
    "        next_word_logits = outputs.logits[:, num_prompts:-1, :]\n",
    "        true_next_tokens = batch['input_ids'][:, num_prompts + 1:]\n",
    "        loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n",
    "    \n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    step_time = (time.time() - step_start) * 1000\n",
    "    \n",
    "    print(f\"{step:<8} {loss.item():<12.4f} {step_time:.1f}\")\n",
    "    \n",
    "    if loss.item() <= 0.05:  \n",
    "        print(f\"\\n Reached excellent loss at step {step}!\")\n",
    "        break\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"=\"*35)\n",
    "print(f\"\\n Training complete!\")\n",
    "print(f\"  Total time: {total_time:.1f}s\")\n",
    "print(f\" Final loss: {loss.item():.4f}\")\n",
    "print(f\" Throughput: {step/total_time:.1f} steps/sec\\n\")\n",
    "\n",
    "assert loss.item() <= 0.1\n",
    "print(\"Good job!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway! Besides, that\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'A quick brown fox'\n",
    "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "\n",
    "space_for_prompts = torch.full(\n",
    "    [batch['input_ids'].shape[0], num_prompts],\n",
    "    fill_value=tokenizer.pad_token_id,\n",
    "    dtype=torch.int64,\n",
    "    device=device\n",
    ")\n",
    "batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n",
    "batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(20):  \n",
    "        next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "        batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "        batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "\n",
    "output = tokenizer.decode(batch['input_ids'][0, num_prompts:].cpu().numpy().tolist())\n",
    "print(f\"Output: {output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using HuggingFace PEFT (2 points)\n",
    "\n",
    "[`peft`](https://huggingface.co/docs/peft/index) is a transformer's sister library that allows you to apply various __p__arameter __e__fficient __f__ine-__t__uning methods to pre-trained transformers. The library imlements both prompt tuning, prefix tuning, as well as several adapter-based techniques under a common interface:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes>=0.46.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.18.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from peft) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.11/site-packages (from peft) (2.10.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (from peft) (5.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from peft) (4.66.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from peft) (1.12.0)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.11/site-packages (from peft) (0.7.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /opt/conda/lib/python3.11/site-packages (from peft) (1.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (2026.2.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /opt/conda/lib/python3.11/site-packages (from cuda-bindings==12.9.4->torch>=1.13.0->peft) (1.3.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers->peft) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.11/site-packages (from transformers->peft) (0.22.2)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.25.0->peft) (4.0.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.25.0->peft) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.25.0->peft) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.25.0->peft) (3.4)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.25.0->peft) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from typer-slim->huggingface_hub>=0.25.0->peft) (8.3.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub>=0.25.0->peft) (1.3.0)\n",
      "Downloading peft-0.18.1-py3-none-any.whl (556 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m557.0/557.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "Successfully installed peft-0.18.1\n"
     ]
    }
   ],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 291/291 [00:03<00:00, 84.11it/s, Materializing param=model.norm.weight]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 131072\n",
      "Total parameters (excluding quantization): 3500544000\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",         \n",
    "    bnb_4bit_use_double_quant=True,    \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 \n",
    ")\n",
    "\n",
    "# del model\n",
    "torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "model_name = 'Enoch/llama-7b-hf'\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "import peft\n",
    "\n",
    "peft_config = peft.PromptTuningConfig(\n",
    "    task_type=peft.TaskType.CAUSAL_LM,\n",
    "    num_virtual_tokens=32,  \n",
    "    prompt_tuning_init=peft.PromptTuningInit.RANDOM,\n",
    ")\n",
    "\n",
    "model = peft.get_peft_model(model, peft_config)\n",
    "print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "print(\"Total parameters (excluding quantization):\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 131072\n",
      "Step     Loss         Time (ms)\n",
      "0        7.0883       197.6\n",
      "5        4.9757       141.5\n",
      "10       3.3164       140.1\n",
      "15       1.8947       141.2\n",
      "20       0.8040       140.6\n",
      "25       0.2368       137.4\n",
      "30       0.0485       162.5\n",
      "\n",
      " Loss reached 0.0485 at step 30\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, PromptTuningConfig, TaskType\n",
    "\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    num_virtual_tokens=32\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "opt = torch.optim.AdamW(trainable_params, lr=1e-2, fused=True)\n",
    "\n",
    "num_params = sum(p.numel() for p in trainable_params)\n",
    "print(f\"Total trainable parameters: {num_params}\")\n",
    "if num_params == 0:\n",
    "    raise ValueError(\"No trainable parameters found! Check your PEFT setup.\")\n",
    "\n",
    "model.train()\n",
    "print(f\"{'Step':<8} {'Loss':<12} {'Time (ms)'}\")\n",
    "\n",
    "for step in range(150):\n",
    "    step_start = time.time()\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    \n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        outputs = model(**batch, labels=batch['input_ids'])\n",
    "        loss = outputs.loss\n",
    "\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    step_time = (time.time() - step_start) * 1000\n",
    "    \n",
    "    if step % 5 == 0: \n",
    "        print(f\"{step:<8} {loss.item():<12.4f} {step_time:.1f}\")\n",
    "    \n",
    "    if loss.item() <= 0.05:\n",
    "        print(f\"\\n Loss reached {loss.item():.4f} at step {step}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway! Besides, that\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'A quick brown fox'\n",
    "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(20):\n",
    "        next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n",
    "        batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "        batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "\n",
    "output = tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist())\n",
    "print(f\"Output: {output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  LoRA: Low-Rank Adaptation (2 points)\n",
    "\n",
    "LoRA adds trainable low-rank matrices in parallel with frozen weights.\n",
    "\n",
    "<center><img src=\"https://i.imgur.com/6bQLNiG.png\" width=240px></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  12884 MiB |  12897 MiB |  26185 MiB |  13301 MiB |\n",
      "|       from large pool |  12884 MiB |  12884 MiB |  25734 MiB |  12850 MiB |\n",
      "|       from small pool |      0 MiB |     13 MiB |    451 MiB |    450 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  12884 MiB |  12897 MiB |  26185 MiB |  13301 MiB |\n",
      "|       from large pool |  12884 MiB |  12884 MiB |  25734 MiB |  12850 MiB |\n",
      "|       from small pool |      0 MiB |     13 MiB |    451 MiB |    450 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  12884 MiB |  12897 MiB |  26182 MiB |  13297 MiB |\n",
      "|       from large pool |  12884 MiB |  12884 MiB |  25734 MiB |  12850 MiB |\n",
      "|       from small pool |      0 MiB |     13 MiB |    447 MiB |    447 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  12888 MiB |  12900 MiB |  12900 MiB |  12288 KiB |\n",
      "|       from large pool |  12884 MiB |  12884 MiB |  12884 MiB |      0 KiB |\n",
      "|       from small pool |      4 MiB |     16 MiB |     16 MiB |  12288 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   3573 KiB |  12602 MiB |  13093 MiB |  13089 MiB |\n",
      "|       from large pool |      0 KiB |  12602 MiB |  12603 MiB |  12603 MiB |\n",
      "|       from small pool |   3573 KiB |     12 MiB |    489 MiB |    486 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     297    |     388    |   29451    |   29154    |\n",
      "|       from large pool |     227    |     227    |     228    |       1    |\n",
      "|       from small pool |      70    |     161    |   29223    |   29153    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     297    |     388    |   29451    |   29154    |\n",
      "|       from large pool |     227    |     227    |     228    |       1    |\n",
      "|       from small pool |      70    |     161    |   29223    |   29153    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       4    |      10    |      10    |       6    |\n",
      "|       from large pool |       2    |       2    |       2    |       0    |\n",
      "|       from small pool |       2    |       8    |       8    |       6    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       3    |      28    |   13347    |   13344    |\n",
      "|       from large pool |       0    |       1    |       2    |       2    |\n",
      "|       from small pool |       3    |      28    |   13345    |   13342    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, module, rank):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        \n",
    "        self.in_features = module.in_features if hasattr(module, 'in_features') else module.weight.shape[1]\n",
    "        self.out_features = module.out_features if hasattr(module, 'out_features') else module.weight.shape[0]\n",
    "        \n",
    "        self.adapter_dtype = torch.bfloat16 \n",
    "        device = next(module.parameters()).device\n",
    "        \n",
    "        self.adapter_A = torch.nn.Parameter(\n",
    "            torch.randn(self.in_features, rank, device=device, dtype=self.adapter_dtype) * 0.02\n",
    "        )\n",
    "        self.adapter_B = torch.nn.Parameter(\n",
    "            torch.zeros(rank, self.out_features, device=device, dtype=self.adapter_dtype)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.module(x) + (x.to(self.adapter_dtype) @ self.adapter_A) @ self.adapter_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "test_linear = nn.Linear(128, 128)\n",
    "test_linear.weight.data[...] = torch.eye(128)\n",
    "test_adapter = LoRALayer(test_linear, rank=8)\n",
    "\n",
    "assert torch.allclose(test_adapter(torch.ones(1, 1, 128)), test_linear.bias + 1), \"please check your forward pass\"\n",
    "\n",
    "test_adapter.adapter_A.data[...] = torch.linspace(0.1, -0.5, 128 * 8).view(128, 8)\n",
    "test_adapter.adapter_B.data[...] = torch.linspace(0.5, -0.1, 128 * 8).view(8, 128)\n",
    "test_linear.bias.data[...] = torch.linspace(1., -1., 128)\n",
    "\n",
    "dummy_loss = F.mse_loss(test_adapter(torch.ones(1, 128) / 128).squeeze(), torch.linspace(-1, 1, 128))\n",
    "assert torch.allclose(dummy_loss, torch.tensor(1.3711389), rtol=0, atol=1e-4), \"Loss calculation failed\"\n",
    "\n",
    "dummy_loss.backward()\n",
    "assert all(w.grad is not None for w in [test_adapter.adapter_A, test_adapter.adapter_B]), \"some adapter weights have no grad\"\n",
    "assert torch.allclose(test_adapter.adapter_A.grad.sum(), torch.tensor(-0.60158), rtol=0, atol=1e-4), \"Grad A wrong\"\n",
    "assert torch.allclose(test_adapter.adapter_B.grad.sum(), torch.tensor(0.9931), rtol=0, atol=1e-4), \"Grad B wrong\"\n",
    "\n",
    "del dummy_loss, test_linear, test_adapter\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply LoRA to the model\n",
    "\n",
    "The code below applies LoRA adapters on top of Q/K/V linear layers in Llama attention. You may also choose to modify other layers:\n",
    "* self_attn.o_proj - attention output projection\n",
    "* mlp.up_proj, mlp.gate_proj, mlp.down_proj - transformer feedforward layers\n",
    "* lm_head - output LM head\n",
    "\n",
    "__Note:__ please scroll down for the homework task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_rank = 8\n",
    "\n",
    "for i, layer in enumerate(model.model.layers):\n",
    "    target_modules = {\n",
    "        'q_proj': layer.self_attn.q_proj,\n",
    "        'k_proj': layer.self_attn.k_proj,\n",
    "        'v_proj': layer.self_attn.v_proj\n",
    "    }\n",
    "    \n",
    "    for name, module in target_modules.items():\n",
    "        if not hasattr(module, 'adapter_A'):\n",
    "            wrapped_layer = LoRALayer(module, rank=lora_rank)\n",
    "            \n",
    "            if name == 'q_proj': layer.self_attn.q_proj = wrapped_layer\n",
    "            elif name == 'k_proj': layer.self_attn.k_proj = wrapped_layer\n",
    "            elif name == 'v_proj': layer.self_attn.v_proj = wrapped_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6269/2385816259.py:3: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n"
     ]
    }
   ],
   "source": [
    "batch = tokenizer(\"Testing gradient flow\", return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "\n",
    "with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "    out = model.forward(**batch)\n",
    "    (out.logits.norm() / 100).backward()\n",
    "\n",
    "for i, module in enumerate(model.modules()):\n",
    "    if isinstance(module, LoRALayer):\n",
    "        assert module.adapter_B.grad is not None, f\"Gradient missing in layer {i}!\"\n",
    "        assert module.adapter_B.grad.norm().item() > 0, f\"Zero gradient in layer {i}!\"\n",
    "\n",
    "model.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (example) How to train your model\n",
    "\n",
    "The example below shows how to train the LoRA adapters on a dummy dataset. You will need to run a _similar_ training task later.\n",
    "\n",
    "__Note:__ please scroll down for the homework task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.4.2)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-23.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-3.0.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m591.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.28.1)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
      "Collecting fsspec<=2025.10.0,>=2023.1.0 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (1.4.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (3.4)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.1/75.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n",
      "Downloading datasets-4.5.0-py3-none-any.whl (515 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m201.0/201.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-23.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.5/47.5 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.3-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-3.0.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (231 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m231.1/231.1 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m246.3/246.3 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (210 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (365 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m365.8/365.8 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, tqdm, requests, pyarrow, propcache, multidict, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2026.2.0\n",
      "    Uninstalling fsspec-2026.2.0:\n",
      "      Successfully uninstalled fsspec-2026.2.0\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 datasets-4.5.0 dill-0.4.0 frozenlist-1.8.0 fsspec-2025.10.0 multidict-6.7.1 multiprocess-0.70.18 pandas-3.0.0 propcache-0.4.1 pyarrow-23.0.0 requests-2.32.5 tqdm-4.67.3 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 2434.97 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:20, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.600244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.211341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.747742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.277206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.360437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.037606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.171049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.726156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.335734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.806300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.065428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.470368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.791745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.362275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.614249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.821897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.887560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.580125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.060308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.704760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.937200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.525858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.247132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.069512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.732091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.252589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.762593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.584977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.086574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.221255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.367807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.067970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.168340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.786722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.821134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.026826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.372652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.377175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.623059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.432425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.103755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.063293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.626655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.524482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.533710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.241752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.265556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.500254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.423653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.042251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.442560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.731141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.665344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.439378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.851255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.061779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.769294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.942526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.177001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.516666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.592540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.171511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.073783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.223957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.049082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.945210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.730516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.892760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.876543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.675200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.668912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.524883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.385924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.499176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.487206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.592440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.925660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.154169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.906606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.132583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.764573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.799520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.208402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.618857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.488470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.499121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.237507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.542831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.074149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.271173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.330561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.582071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.798662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.199998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.367655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.587458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.364145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.300246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.464587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.560692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.6858807059749961, metrics={'train_runtime': 21.1666, 'train_samples_per_second': 9.449, 'train_steps_per_second': 4.724, 'total_flos': 626893241204736.0, 'train_loss': 0.6858807059749961, 'epoch': 6.25})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "data = datasets.load_dataset(\"Abirate/english_quotes\", split=\"train[:32]\") \n",
    "data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)\n",
    "model._hf_peft_config_loaded = True  \n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model, \n",
    "    train_dataset=data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2, \n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=100, \n",
    "        max_steps=100, \n",
    "        learning_rate=2e-4, \n",
    "        bf16=True,            \n",
    "        report_to=\"none\",     \n",
    "        save_strategy=\"no\",   \n",
    "        logging_steps=1, \n",
    "        output_dir='outputs'\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "model.config.use_cache = False \n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# NOTE: this is just an example! you do not have to wait for this progressbar to finish :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final task: *actually* train the model (4 points)\n",
    "\n",
    "Your task is to fine-tune the model to _generate python code_. Please use the above examples for inspiration. More specifically,\n",
    "\n",
    "* __dataset:__ use [codeparrot-clean](https://huggingface.co/datasets/codeparrot/codeparrot-clean) or any other data containing python code. Since you do not need much data for this excercise, it is enough to use just shorter validation subset of `codeparrots`\n",
    "* __preprocessing:__ select python code based on file extentions (.py)  (may skip in case of codeparrot - it is 100% python)\n",
    "* __short lines:__ please take the first 512 characters of each line\n",
    "* __adapter type:__ please use LoRA as defined above __plus at least one of:__\n",
    "   - extra adapter on lm_head\n",
    "   - extra adapter on MLP components (mlp.*)\n",
    "   - trainable input embeddings (requires tweaking memory usage)\n",
    "\n",
    "* __training:__ you do not have to train to convergence. If all goes well, your model should `.generate` code after 500 steps. Please use batch size of at least 4 (4 x 1 x 512 tokens) using `gradient_accumulation_steps=4`.\n",
    "\n",
    "\n",
    "Note: the peft library also has LoRA implementation. However, we ask that for this assignment you show at least one complete training run with your own LoRA code.\n",
    "\n",
    "__Alternative assignment:__ Instead of doing python code, feel free to substitute the task with any other dataset, e.g. your favorite artist or podcast, as long as it's ethical. If you choose your own task, please show examples of what your model learned - or did not learn, akin to the code examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: '<empty string>'\n",
      "Output: The 2018-2019 school year is off to a great start! We are excited to welcome our new students and families to the school. We are also excited to welco...\n",
      "\n",
      "Prompt: 'import'\n",
      "Output: import of the 1990s.\n",
      "The 1990s were a time of great change in the world of fashion. The 1980s were a time of excess, and the 1990s were a time of mini...\n",
      "\n",
      "Prompt: 'from'\n",
      "Output: from the 1960s.\n",
      "The 1960s were a time of great change in the United States. The Civil Rights Movement was in full swing, and the Vietnam War was ragin...\n",
      "\n",
      "Prompt: 'while'\n",
      "Output: while the other is a 100% cotton.\n",
      "The 100% cotton is a bit more expensive, but it is also more durable.\n",
      "The 100% cotton is also a bit more breathable,...\n",
      "\n",
      "Prompt: 'try'\n",
      "Output: try to get the best out of the players.\n",
      "‚ÄúI‚Äôm not going to be a manager who is going to be shouting and screaming at the players.\n",
      "‚ÄúI‚Äôm going to be a ma...\n",
      "\n",
      "Prompt: 'if'\n",
      "Output: if you are a fan of the genre.\n",
      "The game is a 2D platformer, and it is a very good one. The game is very well designed, and the controls are very respo...\n",
      "\n",
      "Prompt: 'for'\n",
      "Output: for the first time in 2017.\n",
      "The 2017 edition of the event will be held from 10 to 12 November 2017 at the Dubai International Convention and Exhibitio...\n",
      "\n",
      "Prompt: 'torch'\n",
      "Output: torch, and the torch is a symbol of the light of the Holy Spirit.\n",
      "The light of the Holy Spirit is the light of truth. The light of the Holy Spirit is ...\n",
      "\n",
      "Prompt: 'def'\n",
      "Output: defendant's motion to dismiss the indictment.\n",
      "The indictment charged that the defendant, on or about August 1, 1958, did unlawfully, wilfully and know...\n",
      "\n",
      "Prompt: 'class'\n",
      "Output: classroom.\n",
      "The first step in the process is to create a classroom culture that is conducive to learning. This means that the teacher must be able to c...\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.gradient_checkpointing_disable() \n",
    "model.config.use_cache = True \n",
    "\n",
    "prompts = ['', 'import', 'from', 'while', 'try', 'if', 'for', 'torch', 'def', 'class']\n",
    "baseline_outputs = {}\n",
    "\n",
    "for prompt in prompts:\n",
    "    if prompt == '':\n",
    "        input_ids = torch.tensor([[tokenizer.bos_token_id]], device=model.device)\n",
    "        attention_mask = torch.tensor([[1]], device=model.device)\n",
    "        batch = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "    else:\n",
    "        batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            for i in range(60):\n",
    "                outputs = model(**batch)\n",
    "                \n",
    "                next_token_logits = outputs.logits[0, -1, :]\n",
    "                next_token = next_token_logits.argmax(-1).reshape(1, 1)\n",
    "                \n",
    "                batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "                batch['attention_mask'] = torch.cat([\n",
    "                    batch['attention_mask'], \n",
    "                    torch.ones_like(next_token)\n",
    "                ], dim=-1)\n",
    "    \n",
    "    output_text = tokenizer.decode(batch['input_ids'][0], skip_special_tokens=True)\n",
    "    baseline_outputs[prompt] = output_text\n",
    "    \n",
    "    display_prompt = prompt if prompt else \"<empty string>\"\n",
    "    print(f\"\\nPrompt: '{display_prompt}'\")\n",
    "    print(f\"Output: {output_text[:150]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data collection...\n",
      "Collected 100 samples...\n",
      "Collected 200 samples...\n",
      "Collected 300 samples...\n",
      "Collected 400 samples...\n",
      "Collected 500 samples...\n",
      "Collected 600 samples...\n",
      "Collected 700 samples...\n",
      "Collected 800 samples...\n",
      "Collected 900 samples...\n",
      "Collected 1000 samples...\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import hashlib\n",
    "\n",
    "code_data = datasets.load_dataset(\n",
    "    \"huggingface-course/codeparrot-ds-train\", \n",
    "    split=\"train\", \n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "code_samples = []\n",
    "seen_hashes = set()\n",
    "target_samples = 1000 \n",
    "\n",
    "print(\"Starting data collection...\")\n",
    "\n",
    "for entry in code_data:\n",
    "    content = entry['content']\n",
    "    \n",
    "    content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    if content_hash not in seen_hashes:\n",
    "        seen_hashes.add(content_hash)\n",
    "        code_samples.append(content)\n",
    "        \n",
    "        if len(code_samples) % 100 == 0:\n",
    "            print(f\"Collected {len(code_samples)} samples...\")\n",
    "            \n",
    "    if len(code_samples) >= target_samples:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 2:05:46, Epoch 1/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.258569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.268279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.210268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.253846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.225961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.246376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.195075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.225357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.282946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.276480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.269822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.194549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.209019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.211979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.238330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.290018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.202623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.215288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.272555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.268863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.235631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.239203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.243187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.229068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.244951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.219169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.243160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.250175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.272294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.227364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.235549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.245805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.276587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.237591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.192568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.262016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.187948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.247796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.232035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.249527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.248716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.249946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.263769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.237490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.236213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.202209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.264125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.239521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.236691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.234525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=1.240020746231079, metrics={'train_runtime': 7568.8193, 'train_samples_per_second': 1.057, 'train_steps_per_second': 0.066, 'total_flos': 1.6232586142733107e+17, 'train_loss': 1.240020746231079, 'epoch': 1.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"PYTORCH_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"content\"], \n",
    "        truncation=True, \n",
    "        max_length=512, \n",
    "        padding=False    \n",
    "    )\n",
    "\n",
    "tokenized_data = code_data.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=[\"content\"] \n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_data, \n",
    "    args=transformers.TrainingArguments(\n",
    "        max_steps=500,                   \n",
    "        per_device_train_batch_size=2,   \n",
    "        gradient_accumulation_steps=8,    \n",
    "        learning_rate=2e-4,\n",
    "        bf16=True,\n",
    "        optim=\"adamw_torch_fused\",\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_num_workers=0, \n",
    "        logging_steps=10,\n",
    "        output_dir='outputs/python_code',\n",
    "        save_strategy=\"no\",              \n",
    "        report_to=\"none\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: '<empty>'\n",
      "Output: The 2018-2019 school year is off to a great start! We are excited to welcome our new students and families to the school. We are also excited to welco...\n",
      "\n",
      "Prompt: 'import'\n",
      "Output: import of the 1990s.\n",
      "The 1990s were a time of great change in the world of fashion. The 1980s were a time of excess, and the 1990s were a time of mini...\n",
      "\n",
      "Prompt: 'from'\n",
      "Output: from the 1960s.\n",
      "The 1960s were a time of great change in the United States. The Civil Rights Movement was in full swing, and the Vietnam War was ragin...\n",
      "\n",
      "Prompt: 'while'\n",
      "Output: while the other is a 100% cotton.\n",
      "The 100% cotton is a bit more expensive, but it is also more durable.\n",
      "The 100% cotton is also a bit more breathable,...\n",
      "\n",
      "Prompt: 'try'\n",
      "Output: try to get the best out of the players.\n",
      "‚ÄúI‚Äôm not going to be a manager who is going to be shouting and screaming at the players.\n",
      "‚ÄúI‚Äôm going to be a ma...\n",
      "\n",
      "Prompt: 'if'\n",
      "Output: if you are a fan of the genre.\n",
      "The game is a 2D platformer, and it is a very good one. The game is very well designed, and the controls are very respo...\n",
      "\n",
      "Prompt: 'for'\n",
      "Output: for the first time in 2017.\n",
      "The 2017 edition of the event will be held from 10 to 12 November 2017 at the Dubai International Convention and Exhibitio...\n",
      "\n",
      "Prompt: 'torch'\n",
      "Output: torch, and the torch is a symbol of the light of the Holy Spirit.\n",
      "The light of the Holy Spirit is the light of truth. The light of the Holy Spirit is ...\n",
      "\n",
      "Prompt: 'def'\n",
      "Output: defendant's motion to dismiss the indictment.\n",
      "The indictment charged that the defendant, on or about August 1, 1958, did unlawfully, wilfully and know...\n",
      "\n",
      "Prompt: 'class'\n",
      "Output: classroom.\n",
      "The first step in the process is to create a classroom culture that is conducive to learning. This means that the teacher must be able to c...\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.config.use_cache = True\n",
    "\n",
    "finetuned_outputs = {}\n",
    "\n",
    "for prompt in prompts:\n",
    "    if not prompt:\n",
    "        input_ids = torch.tensor([[tokenizer.bos_token_id or 1]], device=model.device)\n",
    "        attention_mask = torch.tensor([[1]], device=model.device)\n",
    "        batch = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "    else:\n",
    "        batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            for i in range(60):\n",
    "                outputs = model(**batch)\n",
    "                \n",
    "                next_token_logits = outputs.logits[0, -1, :]\n",
    "                next_token = next_token_logits.argmax(-1).reshape(1, 1)\n",
    "                \n",
    "                batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "                batch['attention_mask'] = torch.cat([\n",
    "                    batch['attention_mask'], \n",
    "                    torch.ones_like(next_token)\n",
    "                ], dim=-1)\n",
    "                \n",
    "                del outputs\n",
    "    \n",
    "    output_text = tokenizer.decode(batch['input_ids'][0], skip_special_tokens=True)\n",
    "    finetuned_outputs[prompt] = output_text\n",
    "    \n",
    "    print(f\"\\nPrompt: '{prompt if prompt else '<empty>'}'\")\n",
    "    print(f\"Output: {output_text[:150]}...\")\n",
    "    \n",
    "    del batch\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 291/291 [00:02<00:00, 110.69it/s, Materializing param=model.norm.weight]                              \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 07:43, Epoch 1/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.250538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.207693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.118477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.156401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.117085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.142845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.087480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.111781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.165279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.162860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.142922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.063845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.091273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.081113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.114309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.152948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.074519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.078287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.146036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.150452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "GENERATION RESULTS\n",
      "==================================================\n",
      "Prompt: '<empty>' -> Ôªøusing System; using System.Collections.Generic; using System.Linq; using System.Text; using System.Threading.Tasks;  na...\n",
      "Prompt: 'import' -> import numpy as np import matplotlib.pyplot as plt  from sklearn.datasets import load_iris from sklearn.ensemble import ...\n",
      "Prompt: 'from' -> from __future__ import print_function  import numpy as np import matplotlib.pyplot as plt from matplotlib import cm from...\n",
      "Prompt: 'while' -> while __pyx_t_1 = (((__pyx_v_info->flags & __pyx_v_info->flags_valid) != 0) != 0);             __pyx_t_2 = __pyx_t_1;   ...\n",
      "Prompt: 'try' -> try sys.settrace()   def _get_trace_func():     \"\"\"     Returns the trace function to use.     \"\"\"     try:         impo...\n",
      "Prompt: 'if' -> if __name__ == '__main__':      import sys     import os     import numpy as np     import matplotlib.pyplot as plt     ...\n",
      "Prompt: 'for' -> for __pyx_t_1 = 0; __pyx_t_1 < __pyx_t_2; __pyx_t_1+=1) { #if CYTHON_COMPILING_IN_CPYTHON    #...\n",
      "Prompt: 'torch' -> torch. The 1990s saw the rise of the Internet, and the 2000s saw the rise of the smartphone. The 2010s have seen the ris...\n",
      "Prompt: 'def' -> def __init__(self, *args, **kwargs):         super(TestCase, self).__init__(*args, **kwargs)          self.assertTrue(se...\n",
      "Prompt: 'class' -> class - the first of its kind in the UK - will be held at the University of Birmingham on 15-16 September 2016. The cour...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import hashlib\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import LlamaTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/opt/conda/lib/'\n",
    "os.environ[\"PYTORCH_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "model_name = 'Enoch/llama-7b-hf'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name, legacy=True, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 \n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, module, rank):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.in_features = module.in_features if hasattr(module, 'in_features') else module.weight.shape[1]\n",
    "        self.out_features = module.out_features if hasattr(module, 'out_features') else module.weight.shape[0]\n",
    "        \n",
    "        self.dtype = torch.bfloat16 \n",
    "        target_device = next(module.parameters()).device\n",
    "        \n",
    "        self.adapter_A = torch.nn.Parameter(\n",
    "            torch.randn(self.in_features, rank, device=target_device, dtype=self.dtype) * 0.02\n",
    "        )\n",
    "        self.adapter_B = torch.nn.Parameter(\n",
    "            torch.zeros(rank, self.out_features, device=target_device, dtype=self.dtype)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.module(x) + (x.to(self.dtype) @ self.adapter_A) @ self.adapter_B\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for layer in model.model.layers:\n",
    "    if not hasattr(layer.self_attn.q_proj, 'adapter_A'):\n",
    "        layer.self_attn.q_proj = LoRALayer(layer.self_attn.q_proj, rank=8)\n",
    "        layer.self_attn.v_proj = LoRALayer(layer.self_attn.v_proj, rank=8)\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "    if \"adapter_\" in n:\n",
    "        p.requires_grad = True\n",
    "\n",
    "model.enable_input_require_grads()\n",
    "model.gradient_checkpointing_enable()\n",
    "model.is_quantized = False\n",
    "model.peft_config = {\"dummy\": \"dummy\"} \n",
    "\n",
    "raw_data = datasets.load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\", streaming=True)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"content\"], truncation=True, max_length=512, padding=False)\n",
    "\n",
    "tokenized_data = raw_data.map(tokenize_function, batched=True, remove_columns=[\"content\"])\n",
    "\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=2e-4, fused=True)\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_data, \n",
    "    args=transformers.TrainingArguments(\n",
    "        max_steps=200,                   \n",
    "        per_device_train_batch_size=2,   \n",
    "        gradient_accumulation_steps=8,    \n",
    "        learning_rate=2e-4,\n",
    "        bf16=True,                       \n",
    "        optim=\"adamw_torch_fused\",\n",
    "        dataloader_num_workers=0,        \n",
    "        logging_steps=10,\n",
    "        output_dir='outputs/python_code',\n",
    "        save_strategy=\"no\",              \n",
    "        report_to=\"none\"\n",
    "    ),\n",
    "    optimizers=(optimizer, None),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()\n",
    "\n",
    "model.eval()\n",
    "model.gradient_checkpointing_disable()\n",
    "model.config.use_cache = True\n",
    "\n",
    "prompts = ['', 'import', 'from', 'while', 'try', 'if', 'for', 'torch', 'def', 'class']\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for prompt in prompts:\n",
    "    if not prompt:\n",
    "        bos_id = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else 1\n",
    "        input_ids = torch.tensor([[bos_id]], device=model.device)\n",
    "        attention_mask = torch.tensor([[1]], device=model.device)\n",
    "        batch = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "    else:\n",
    "        batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            for i in range(60):\n",
    "                outputs = model(**batch)\n",
    "                next_token = outputs.logits[0, -1, :].argmax(-1).reshape(1, 1)\n",
    "                batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n",
    "                batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n",
    "                del outputs\n",
    "    \n",
    "    output_text = tokenizer.decode(batch['input_ids'][0], skip_special_tokens=True)\n",
    "    \n",
    "    clean_text = output_text[:120].replace('\\n', ' ')\n",
    "    disp_p = prompt if prompt else \"<empty>\"\n",
    "    print(f\"Prompt: '{disp_p}' -> {clean_text}...\")\n",
    "    \n",
    "    del batch\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table style=\"width: 100%; border-collapse: collapse; font-family: sans-serif; border: 1px solid #bdc3c7;\">\n",
       "    <thead>\n",
       "        <tr style=\"background-color: #2c3e50; color: #ffffff;\">\n",
       "            <th style=\"padding: 12px; text-align: left; width: 10%;\">PROMPT</th>\n",
       "            <th style=\"padding: 12px; text-align: left; width: 45%;\">BEFORE TRAINING</th>\n",
       "            <th style=\"padding: 12px; text-align: left; width: 45%;\">AFTER TRAINING (H200)</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr style=\"background-color: #ecf0f1;\">\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; font-weight: bold; vertical-align: top;\">''</td>\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top;\">The 2018-2019 school year is off to a great start! We are excited to welcome our new students and families to the school. We are also excited to welco...</td>\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top; background-color: #e8f8f5;\">\n",
       "\n",
       "### Liens externes\n",
       "\n",
       "* (en) R√©f√©rence Animal Diversity Web : Acanthoscurus (consult√© le 19 septembre 2014)\n",
       "* (en) R√©f√©rence Catalogue of Life : Acanthoscurus   (consult√© le 12</td>\n",
       "    </tr><tr style=\"background-color: #ffffff;\">\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; font-weight: bold; vertical-align: top;\">'import'</td>\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top;\">import of the 1990s.\n",
       "The 1990s were a time of great change in the world of fashion. The 1980s were a time of excess, and the 1990s were a time of mini...</td>\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top; background-color: #e8f8f5;\">import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "from sklearn.datasets import load_iris\n",
       "from sklearn.ensemble import RandomForestClassifier\n",
       "from sklearn.metrics import confusion_matrix\n",
       "\n",
       "# Load the iris dataset\n",
       "iris = load</td>\n",
       "    </tr><tr style=\"background-color: #ecf0f1;\">\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; font-weight: bold; vertical-align: top;\">'from'</td>\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top;\">from the 1960s.\n",
       "The 1960s were a time of great change in the United States. The Civil Rights Movement was in full swing, and the Vietnam War was ragin...</td>\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top; background-color: #e8f8f5;\">from __future__ import division\n",
       "from __future__ import print_function\n",
       "from __future__ import unicode_literals\n",
       "\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "\n",
       "from sklearn.datasets import load_iris\n",
       "from sklearn.ensemble</td>\n",
       "    </tr><tr style=\"background-color: #ffffff;\">\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; font-weight: bold; vertical-align: top;\">'while'</td>\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top;\">while the other is a 100% cotton.\n",
       "The 100% cotton is a bit more expensive, but it is also more durable.\n",
       "The 100% cotton is also a bit more breathable,...</td>\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top; background-color: #e8f8f5;\">while __pyx_t_1 = (((__pyx_v_info-&gt;flags & __pyx_v_info-&gt;flags_valid) != 0) != 0);\n",
       "            __pyx_t_2 = __pyx_t_1;\n",
       "         </td>\n",
       "    </tr><tr style=\"background-color: #ecf0f1;\">\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; font-weight: bold; vertical-align: top;\">'try'</td>\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top;\">try to get the best out of the players.\n",
       "‚ÄúI‚Äôm not going to be a manager who is going to be shouting and screaming at the players.\n",
       "‚ÄúI‚Äôm going to be a ma...</td>\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top; background-color: #e8f8f5;\">try sys.settrace()\n",
       "\n",
       "\n",
       "def _get_trace_func():\n",
       "    \"\"\"\n",
       "    Returns the trace function to use.\n",
       "    \"\"\"\n",
       "    try:\n",
       "        import ctypes\n",
       "        return ctypes.CDLL('libpython.so.1.0</td>\n",
       "    </tr><tr style=\"background-color: #ffffff;\">\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; font-weight: bold; vertical-align: top;\">'if'</td>\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top;\">if you are a fan of the genre.\n",
       "The game is a 2D platformer, and it is a very good one. The game is very well designed, and the controls are very respo...</td>\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top; background-color: #e8f8f5;\">if __name__ == '__main__':\n",
       "\n",
       "    import sys\n",
       "    import os\n",
       "    import numpy as np\n",
       "    import matplotlib.pyplot as plt\n",
       "    import matplotlib.cm as cm\n",
       "    import matplotlib.colors as colors\n",
       "    import matplotlib.patches as patches\n",
       "</td>\n",
       "    </tr><tr style=\"background-color: #ecf0f1;\">\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; font-weight: bold; vertical-align: top;\">'for'</td>\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top;\">for the first time in 2017.\n",
       "The 2017 edition of the event will be held from 10 to 12 November 2017 at the Dubai International Convention and Exhibitio...</td>\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top; background-color: #e8f8f5;\">for __pyx_t_1 = 0; __pyx_t_1 &lt; __pyx_t_2; __pyx_t_1+=1) {\n",
       "#else\n",
       "#if CYTHON_COMPILING_IN_CPYTHON</td>\n",
       "    </tr><tr style=\"background-color: #ffffff;\">\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; font-weight: bold; vertical-align: top;\">'torch'</td>\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top;\">torch, and the torch is a symbol of the light of the Holy Spirit.\n",
       "The light of the Holy Spirit is the light of truth. The light of the Holy Spirit is ...</td>\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top; background-color: #e8f8f5;\">torch.\n",
       "The 1990s saw the rise of the Internet, and the 2000s saw the rise of the smartphone. The 2010s have seen the rise of the smartwatch.\n",
       "The smartwatch is a device that is worn on the</td>\n",
       "    </tr><tr style=\"background-color: #ecf0f1;\">\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; font-weight: bold; vertical-align: top;\">'def'</td>\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top;\">defendant's motion to dismiss the indictment.\n",
       "The indictment charged that the defendant, on or about August 1, 1958, did unlawfully, wilfully and know...</td>\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top; background-color: #e8f8f5;\">def __init__(self, *args, **kwargs):\n",
       "        super(TestCase, self).__init__(*args, **kwargs)\n",
       "\n",
       "        self.assertTrue(self.assertTrue is self.assertTrue)\n",
       "        self.assertFalse(self.assertFalse is self</td>\n",
       "    </tr><tr style=\"background-color: #ffffff;\">\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; font-weight: bold; vertical-align: top;\">'class'</td>\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top;\">classroom.\n",
       "The first step in the process is to create a classroom culture that is conducive to learning. This means that the teacher must be able to c...</td>\n",
       "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top; background-color: #e8f8f5;\">class - the class that is the parent of the class that is the parent of the class that is the parent of the class that is the parent of the class that is the parent of the class that is the parent of the class that is the parent of the class that is </td>\n",
       "    </tr>\n",
       "    </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "rows = []\n",
    "for i, prompt in enumerate(generation_results['prompts']):\n",
    "    before_raw = generation_results['baseline'].get(prompt, \"No data found\")\n",
    "    after_raw = generation_results['finetuned'].get(prompt, \"No data found\")\n",
    "    \n",
    "    before = before_raw.replace('<', '&lt;').replace('>', '&gt;')[:250]\n",
    "    after = after_raw.replace('<', '&lt;').replace('>', '&gt;')[:250]\n",
    "    \n",
    "    bg = '#ecf0f1' if i % 2 == 0 else '#ffffff'\n",
    "    \n",
    "    row = f'''<tr style=\"background-color: {bg};\">\n",
    "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; font-weight: bold; vertical-align: top;\">'{prompt}'</td>\n",
    "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top;\">{before}</td>\n",
    "        <td style=\"padding: 10px; border: 1px solid #bdc3c7; white-space: pre-wrap; font-size: 11px; vertical-align: top; background-color: #e8f8f5;\">{after}</td>\n",
    "    </tr>'''\n",
    "    rows.append(row)\n",
    "\n",
    "table_html = f'''\n",
    "<table style=\"width: 100%; border-collapse: collapse; font-family: sans-serif; border: 1px solid #bdc3c7;\">\n",
    "    <thead>\n",
    "        <tr style=\"background-color: #2c3e50; color: #ffffff;\">\n",
    "            <th style=\"padding: 12px; text-align: left; width: 10%;\">PROMPT</th>\n",
    "            <th style=\"padding: 12px; text-align: left; width: 45%;\">BEFORE TRAINING</th>\n",
    "            <th style=\"padding: 12px; text-align: left; width: 45%;\">AFTER TRAINING (H200)</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        {\"\".join(rows)}\n",
    "    </tbody>\n",
    "</table>\n",
    "'''\n",
    "\n",
    "display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you reach this: congratulations! you've completed everything in this practice session.\n",
    "\n",
    "If you want to dig deeper, try to implement prompt-tuning (for bonus points!).\n",
    "You can read more about prompt tuning variants in paper [1](https://arxiv.org/abs/2104.08691) or paper [2](https://arxiv.org/abs/2101.00190). Both versions can be implemented by passing trainable prompts as `model.forward(..., past_key_values=your_prompts)`.\n",
    "\n",
    "\n",
    "\n",
    "### Read more\n",
    "\n",
    "* How post-training quantization works: https://arxiv.org/abs/2208.07339\n",
    "* An overview of running large models: https://huggingface.co/docs/accelerate/package_reference/big_modeling\n",
    "* A general library for different adapter types: https://adapterhub.ml/\n",
    "\n",
    "\n",
    "### [extra info] Running other models.\n",
    "\n",
    "This notebook's code can run with other models of similar size, such as [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b), [OPT-6.7B](https://huggingface.co/facebook/opt-6.7b) or [BLOOM-7.1B](https://huggingface.co/bigscience/bloom-7b1). However, they will require minor code tweaks:\n",
    "1. change the model name in `AutoModelForCausalLM.from_pretrained()` __and__ `AutoTokenizer`\n",
    "2. In the prompt tuning code, change `model.model.embed_tokens` to refer to the target model's word embeddings. Simply `print(model)` to navigate to them.\n",
    "3. Change code to add Lora layers - specifically where you what the transformer block components, since those components now have different names."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
